# 第五次课后练习 之二（选做）

在本次作业中，如果你在配置环境中遇到了问题，可以参考助教的解决方案：

首先我们在将这个文件放在一个文件夹中（可以和这门课程的其他作业放在一起，但是别放在桌面这种一大堆其他文件的文件夹里）。

随后运行：
```
python3 -m venv myenv
source myenv/bin/activate
pip install openai
pip install requests
pip install socksio
```

然后在 notebook 中切换 ipykernel 为 myenv（如果你采用的是 VSCode 的话，可以看看右上角的 ipykernel，点一下就可以切换了），就可以运行了。

如果你发现还存在包缺失的情况，注意采用 Restart，不然环境可能同步不过来。

**负责助教：吴迪**

<span style="color:red; font-weight:bold;">请将作业文件命名为 第五次课后练习-之二+姓名+学号.ipynb, 例如 第五次课后练习-之二+张三+1000000000.ipynb</span>

#  请认真阅读代码，理解学习代码的功能

## **0.1** 读取网页内容，调用大语言模型API进行中文摘要
    

```python
import re
import requests
from openai import OpenAI

# 目标URL列表
target_urls = [
    "https://arxiv.org/abs/2410.03761",
    "https://arxiv.org/abs/2305.15186"
]

# OpenRouter API配置，这个是提供免费deepseek服务的站点，
# 也可以替换成直接访问deepseekAPI的版本
API_ENDPOINT = "https://api.deepseek.com"
API_KEY = "sk-960bc12482b24d75bfd716f9c644f4d8"

# 正则表达式，用于解析网页摘要（读取html网页对象的content字段内容）
pattern = r'<meta\s+property="og:description"\s+content="(.*?)"\s*/>'

# 初始化OpenAI客户端
client = OpenAI(
    base_url=API_ENDPOINT,
    api_key=API_KEY,
)

def generate_summary(text: str) -> str:
    """生成文本摘要"""
    try:
        # 调用DeepSeek API生成摘要
        # 如果你采用了其他的站点的内容，你可能需要修改这里的 model 的内容。
        completion = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {
                    "role": "system",
                    "content": "Please write a 300 word(character) summary of the user's text in Chinese"
                },
                {
                    "role": "user",
                    "content": text
                }
            ]
        )
        # 返回生成的摘要内容
        return completion.choices[0].message.content
    except Exception as e:
        print(f"Error generating summary: {e}")
        return "Failed to generate summary"

def fetch_webpage_content(url: str) -> str:
    """获取网页内容"""
    try:
        # 发送HTTP GET请求获取网页内容
        response = requests.get(url, timeout=10)  # 设置超时时间
        response.raise_for_status()  # 检查请求是否成功
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return None

def parse_summary(html_content: str) -> str:
    """从网页内容中解析出摘要"""
    try:
        # 使用正则表达式从HTML中提取摘要，读取html网页对象的content字段
        match = re.search(pattern, html_content)
        if match:
            return match.group(1)
        else:
            raise Exception("No summary found in the webpage")
    except Exception as e:
        print(f"Error parsing summary: {e}")
        return None

def main():
    # 遍历目标URL列表
    
    docs = ""
    
    for url in target_urls:
        print(f"Processing URL: {url}")
        
        # 获取网页内容
        html_content = fetch_webpage_content(url)
        if not html_content:
            print('access error')
            continue  # 如果获取失败，跳过当前URL
        
        # 解析摘要
        summary = parse_summary(html_content)
        if not summary:
            print('parse error')
            continue  # 如果解析失败，跳过当前URL
            
        docs += summary
        
        # 生成中文摘要
        chinese_summary = generate_summary(summary)
        
        # 打印结果
        print(f"Original Summary: {summary}")
        print(f"Chinese Summary: {chinese_summary}\n")

    chinese_summary = generate_summary(docs)
    print(f"Original message: {docs}")
    print(f"multi doc Summary: {chinese_summary}\n")
if __name__ == "__main__":
    main()
```

# 1 仿照上面的框架，实现一个自己版本的多文档（或多轮对话）的分析功能。可以用更复杂的爬虫获得需要的信息。也可以直接从本地读取数据文件实现有实际意义的功能。

完成有新意有创意工作的同学，或在作业过程中觉得有心得或者自己拓展学习到有价值内容的，可以在文件名最后加一个#号。例如第五次课后练习+张三+1000000000+#.ipynb
只是完成学习的同学，没有尝试改进探索工作的可以不提交这个作业

```python
'''
可以从本地文件夹和url读取pdf文件,实现了文档解析、文档比较、文档摘要、并可以生成思维导图展示各个文档间的关系、提取关键主题等等
'''
import os
import PyPDF2
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
import requests
import matplotlib.colors as mcolors
from matplotlib.patches import FancyBboxPatch
import numpy as np
from openai import OpenAI
from typing import List, Dict, Any
import pdfkit
from urllib.parse import urlparse
import mimetypes
# OpenRouter API配置
API_ENDPOINT = "https://api.deepseek.com"
API_KEY = "sk-960bc12482b24d75bfd716f9c644f4d8"

# 初始化OpenAI客户端
client = OpenAI(
    base_url=API_ENDPOINT,
    api_key=API_KEY,
)

class PDFDocumentAnalyzer:
    """PDF文档分析器，用于从多个PDF文件中提取内容并进行分析"""
    
    def __init__(self, pdf_folder_path: str,url_folder_path: str):
        """初始化PDF文档分析器
        
        Args:
            pdf_folder_path: PDF文件夹路径
        """
        self.pdf_folder_path = pdf_folder_path
        self.url_folder_path = url_folder_path
        self.documents = {}  # 存储文档内容
        self.summaries = {}  # 存储文档摘要
        self.comparison_result = None  # 存储比较结果
        self.theme_relationships = {}  # 存储主题间关系
        
  
    
    def load_documents(self, max_pages_per_doc: int = 5) -> None:
        """加载文件夹中的所有PDF文档
        
        Args:
            max_pages_per_doc: 每个文档最多读取的页数
        """
        # 确保文件夹存在
        if not os.path.exists(self.pdf_folder_path) and os.path.exists(self.url_folder_path):
            print(f"请上传有效的文件夹或者url")
            return
            
        # 遍历文件夹下的所有PDF文件
        pdf_files=[f for f in os.listdir(self.pdf_folder_path) if f.lower().endswith('.pdf')]
        if not pdf_files:
            print(f"在文件夹 '{self.pdf_folder_path}' 中未找到PDF文件")
            return
            
        print(f"在{self.pdf_folder_path}找到 {len(pdf_files)} 个PDF文件")
        
        # 读取每个PDF文件的内容
        for pdf_file in pdf_files:
            file_path = os.path.join(self.pdf_folder_path, pdf_file)
            try:
                with open(file_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    
                    # 限制页数以避免处理过大的文档
                    pages_to_read = min(len(reader.pages), max_pages_per_doc)
                    
                    # 提取文本内容
                    content = ""
                    for i in range(pages_to_read):
                        content += reader.pages[i].extract_text()
                    
                    # 存储文档内容
                    self.documents[pdf_file] = content
                    print(f"成功读取 '{pdf_file}'（{pages_to_read}/{len(reader.pages)} 页）")
                    
            except Exception as e:
                print(f"读取 '{pdf_file}' 时出错: {e}")
                
        url_pdf_files=[f for f in os.listdir(self.url_folder_path) if f.lower().endswith('.pdf')]
        if not url_pdf_files:
            print(f"在文件夹 '{self.url_folder_path}' 中未找到PDF文件")
            return
            
        print(f"在{self.url_folder_path}找到 {len(url_pdf_files)} 个PDF文件")
        
        # 读取每个PDF文件的内容
        for pdf_file in url_pdf_files:
            file_path = os.path.join(self.url_folder_path, pdf_file)
            try:
                with open(file_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    
                    # 限制页数以避免处理过大的文档
                    pages_to_read = min(len(reader.pages), max_pages_per_doc)
                    
                    # 提取文本内容
                    content = ""
                    for i in range(pages_to_read):
                        content += reader.pages[i].extract_text()
                    
                    # 存储文档内容
                    self.documents[pdf_file] = content
                    print(f"成功读取 '{pdf_file}'（{pages_to_read}/{len(reader.pages)} 页）")
                    
            except Exception as e:
                print(f"读取 '{pdf_file}' 时出错: {e}")
                
        print(f"成功加载 {len(self.documents)} 个PDF文档")
    
    def generate_document_summaries(self) -> None:
        """为所有加载的文档生成摘要"""
        if not self.documents:
            print("没有可用的文档，请先加载文档")
            return
            
        print(f"正在为{len(self.documents)}个文档生成摘要...")
        
        for doc_name, content in self.documents.items():
            print(f"正在处理 '{doc_name}'...")
            
            # 如果内容太长，截取前面部分
            max_content_length = 60000  # 根据API限制调整
            print(f"{doc_name}的文章有{len(content)}个单词")
            if len(content) > max_content_length:
                print(f"{doc_name}的文章内容太长，截取前{max_content_length}个单词")
                content = content[:max_content_length] + "..."
            
            try:
                # 调用DeepSeek API生成摘要
                completion = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {
                            "role": "system",
                            "content": "请对用户提供的文档内容生成一个1000字的中文摘要，包括主要观点、关键词、方法和结论。"
                        },
                        {
                            "role": "user",
                            "content": content
                        }
                    ]
                )
                
                # 存储生成的摘要
                summary = completion.choices[0].message.content
                self.summaries[doc_name] = summary
                print(f"已生成 '{doc_name}' 的摘要")
                
            except Exception as e:
                print(f"为 '{doc_name}' 生成摘要时出错: {e}")
                
        print(f"已完成 {len(self.summaries)}/{len(self.documents)} 个文档的摘要生成")
    
    def compare_documents(self) -> str:
        """比较所有文档的内容，分析其相似点和差异点"""
        if len(self.summaries) < 2:
            print("需要至少两个文档摘要才能进行比较")
            return None
            
        print("正在比较文档...")
        
        # 准备所有摘要的合并文本
        all_summaries = ""
        for doc_name, summary in self.summaries.items():
            all_summaries += f"文档: {doc_name}\n摘要: {summary}\n\n"
        
        try:
            # 调用DeepSeek API进行文档比较
            completion = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "请对提供的多个文档摘要进行比较分析，指出它们之间的相同点和不同点，并提供一个综合性的分析报告。"
                    },
                    {
                        "role": "user",
                        "content": all_summaries
                    }
                ]
            )
            
            # 存储比较结果
            self.comparison_result = completion.choices[0].message.content
            print("文档比较完成")
            return self.comparison_result
            
        except Exception as e:
            print(f"比较文档时出错: {e}")
            return None
    
    def extract_key_themes(self) -> List[str]:
        """从所有文档中提取关键主题"""
        if not self.documents:
            print("没有可用的文档，请先加载文档")
            return []
            
        # 合并所有摘要
        all_summaries = "\n\n".join(self.summaries.values())
        
        try:
            # 调用DeepSeek API提取关键主题
            completion = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "请从以下文档摘要中提取5-10个关键主题，以简短的词组表示，并以JSON格式返回。格式为: {\"themes\": [\"主题1\", \"主题2\", ...]}"
                    },
                    {
                        "role": "user",
                        "content": all_summaries
                    }
                ]
            )
            
            response = completion.choices[0].message.content
            
            # 尝试解析JSON响应，如果失败则手动提取主题
            try:
                import json
                themes = json.loads(response)["themes"]
            except:
                # 如果JSON解析失败，尝试从文本中提取主题
                themes = [line.strip().strip("\"'-").strip() 
                          for line in response.split("\n") 
                          if line.strip() and not line.startswith("{") and not line.startswith("}")]
                
            return themes
        
        except Exception as e:
            print(f"提取关键主题时出错: {e}")
            return []
    
    def get_detailed_analysis(self, query: str) -> str:
        """根据特定查询对文档进行详细分析
        
        Args:
            query: 分析查询，例如 "分析文档中关于方法论的讨论"
        
        Returns:
            详细分析结果
        """
        if not self.documents:
            print("没有可用的文档，请先加载文档")
            return ""
        
        # 准备用于分析的所有摘要
        all_summaries = ""
        for doc_name, summary in self.summaries.items():
            all_summaries += f"文档: {doc_name}\n摘要: {summary}\n\n"
        
        try:
            # 调用DeepSeek API进行详细分析
            completion = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "你是一名学术分析助手，擅长对文档内容进行深入分析。"
                    },
                    {
                        "role": "user",
                        "content": f"以下是多个文档的摘要:\n\n{all_summaries}\n\n请根据以下问题进行分析: {query}"
                    }
                ]
            )
            
            return completion.choices[0].message.content
            
        except Exception as e:
            print(f"执行详细分析时出错: {e}")
            return ""
    
    def generate_report(self, output_file: str = "document_analysis_report.md") -> None:
        """生成完整的文档分析报告
        
        Args:
            output_file: 输出文件路径
        """
        if not self.documents or not self.summaries:
            print("没有足够的数据生成报告，请先加载文档并生成摘要")
            return
        
        # 提取关键主题
        themes = self.extract_key_themes()
        
        # 如果还没有比较结果，生成一个
        if not self.comparison_result:
            self.compare_documents()
        
        # 创建报告内容
        report = "# 文档分析报告\n\n"
        report += f"## 分析的文档 ({len(self.documents)})\n\n"
        
        for doc_name in self.documents.keys():
            report += f"- {doc_name}\n"
        
        # 生成思维导图
        mind_map_file = "document_mind_map.png"
        if not os.path.exists(mind_map_file):
            self.generate_mind_map(output_file=mind_map_file)
        
        # 在报告中添加思维导图
        report += "\n## 文档关系思维导图\n\n"
        report += f"![文档分析思维导图]({mind_map_file})\n\n"
        report += "*上图展示了文档主题的关系结构及文档与主题的关联*\n\n"
        
        report += "\n## 文档摘要\n\n"
        
        for doc_name, summary in self.summaries.items():
            report += f"### {doc_name}\n\n{summary}\n\n"
        
        report += "## 关键主题\n\n"
        
        for theme in themes:
            report += f"- {theme}\n"
        
        report += "\n## 文档比较分析\n\n"
        report += self.comparison_result if self.comparison_result else "未进行文档比较"
        
        # 添加主题关系描述
        if hasattr(self, 'theme_relationships') and self.theme_relationships:
            report += "\n\n## 主题关系分析\n\n"
            for theme, related_themes in self.theme_relationships.items():
                if related_themes:
                    report += f"### {theme}\n\n"
                    report += f"与以下主题相关：\n\n"
                    for related in related_themes:
                        report += f"- {related}\n"
                    report += "\n"
        
        # 保存报告
        try:
            with open(output_file, "w", encoding="utf-8") as file:
                file.write(report)
            print(f"分析报告已保存至 '{output_file}'")
        except Exception as e:
            print(f"保存报告时出错: {e}")
            print("以下是报告内容:\n")
            print(report)
            
    def analyze_theme_relationships(self) -> Dict[str, List[str]]:
        """分析主题之间的关系
        
        Returns:
            主题关系的字典，格式为 {主题: [相关主题列表]}
        """
        themes = self.extract_key_themes()
        if not themes or len(themes) < 2:
            print("需要至少两个主题才能分析关系")
            return {}
            
        print("正在分析主题之间的关系...")
        
        # 将主题转换为字符串
        themes_text = ", ".join(themes)
        
        try:
            # 调用DeepSeek API分析主题关系
            completion = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "请分析以下主题之间的关系，并以JSON格式返回结果。格式示例：{\"主题1\": [\"相关主题A\", \"相关主题B\"], \"主题2\": [\"相关主题C\"]}"
                    },
                    {
                        "role": "user",
                        "content": f"基于文档分析，以下是提取的关键主题：{themes_text}。请分析这些主题之间的关联关系。"
                    }
                ]
            )
            
            response = completion.choices[0].message.content
            
            # 尝试解析JSON响应
            try:
                import json
                # 尝试直接解析完整响应
                try:
                    relationships = json.loads(response)
                except:
                    # 如果失败，尝试从文本中提取JSON部分
                    import re
                    json_match = re.search(r'\{.*\}', response, re.DOTALL)
                    if json_match:
                        relationships = json.loads(json_match.group(0))
                    else:
                        raise Exception("无法从响应中提取JSON")
                
                # 验证格式
                self.theme_relationships = {
                    theme: [related for related in related_themes if related in themes]
                    for theme, related_themes in relationships.items()
                    if theme in themes
                }
                
            except Exception as e:
                print(f"解析主题关系时出错: {e}")
                # 创建一个简单的回退关系结构
                self.theme_relationships = {theme: [] for theme in themes}
                
                # 尝试将每个主题与至少一个其他主题关联
                for i, theme in enumerate(themes):
                    if i < len(themes) - 1:
                        self.theme_relationships[theme].append(themes[i + 1])
                    else:
                        self.theme_relationships[theme].append(themes[0])
            
            return self.theme_relationships
                
        except Exception as e:
            print(f"分析主题关系时出错: {e}")
            return {theme: [] for theme in themes}
    def generate_mind_map(self, output_file: str = "document_mind_map.png", 
                        title: str = "文档分析思维导图",
                        figsize: tuple = (16, 12)) -> None:
        """生成基于分析结果的思维导图
        
        Args:
            output_file: 输出文件路径
            title: 思维导图标题
            figsize: 图表尺寸
        """
        if not self.documents:
            print("没有可用的文档，请先加载文档")
            return
            
        # 提取主题并分析关系
        if not hasattr(self, 'theme_relationships') or not self.theme_relationships:
            self.analyze_theme_relationships()
            
        if not self.theme_relationships:
            print("无法生成思维导图：未能提取主题关系")
            return
            
        print("正在生成思维导图...")
        
        # 创建有向图
        G = nx.DiGraph()
        
        # 添加中心节点（根节点）
        center_node = "文档分析"
        G.add_node(center_node)
        
        # 添加主题节点和它们与中心的连接
        themes = list(self.theme_relationships.keys())
        for theme in themes:
            G.add_node(theme)
            G.add_edge(center_node, theme)
            
            # 添加主题间关系
            for related_theme in self.theme_relationships.get(theme, []):
                if related_theme in themes and related_theme != theme:
                    G.add_edge(theme, related_theme)
        
        # 添加文档节点和它们与相关主题的连接
        for doc_name in self.documents.keys():
            # 使用文件名的简短版本作为节点标签
            short_name = doc_name[:25] + "..." if len(doc_name) > 28 else doc_name
            G.add_node(short_name)
            
            # 尝试将文档与最相关的主题连接起来
            doc_connected = False
            
            try:
                # 匹配文档与主题的相关性
                all_themes = ", ".join(themes)
                doc_summary = self.summaries.get(doc_name, "")
                
                completion = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {
                            "role": "system",
                            "content": "请以JSON格式返回该文档与给定主题的相关性评分，格式为：{\"主题1\": 0.8, \"主题2\": 0.4, ...}，数值范围0-1。"
                        },
                        {
                            "role": "user",
                            "content": f"文档摘要：\n{doc_summary[:500]}...\n\n主题列表：{all_themes}\n\n请评估此文档与每个主题的相关性。"
                        }
                    ]
                )
                
                # 解析响应
                import json
                import re
                
                response = completion.choices[0].message.content
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    relevance = json.loads(json_match.group(0))
                    
                    # 找出最相关的两个主题
                    relevant_themes = sorted([(theme, score) for theme, score in relevance.items() 
                                            if theme in themes and score >= 0.5],
                                        key=lambda x: x[1], reverse=True)[:2]
                    
                    for theme, _ in relevant_themes:
                        G.add_edge(theme, short_name)
                        doc_connected = True
                
            except Exception as e:
                print(f"匹配 '{doc_name}' 与主题时出错: {e}")
            
            # 如果未能找到相关主题，则随机连接到一个主题
            if not doc_connected:
                import random
                random_theme = random.choice(themes)
                G.add_edge(random_theme, short_name)
        
        # 设置图形
        plt.figure(figsize=figsize)
        plt.title(title, fontsize=20, fontweight='bold')
        
        # 定义节点颜色和大小
        colors = list(mcolors.TABLEAU_COLORS)
        node_colors = {
            center_node: 'red',
            **{theme: colors[i % len(colors)] for i, theme in enumerate(themes)},
            **{doc[:25] + "..." if len(doc) > 28 else doc: 'lightgray' for doc in self.documents.keys()}
        }
        
        node_sizes = {
            center_node: 3000,
            **{theme: 2000 for theme in themes},
            **{doc[:25] + "..." if len(doc) > 28 else doc: 1500 for doc in self.documents.keys()}
        }
        
        # 使用spring_layout替代graphviz_layout，适用于中文字符
        # 将中心节点固定在中央位置
        pos = nx.spring_layout(G, k=0.3, iterations=50, seed=42)
        
        # 调整中心节点位置
        pos[center_node] = np.array([0, 0])
        
        # 绘制节点
        for node in G.nodes():
            nx.draw_networkx_nodes(
                G, pos, 
                nodelist=[node],
                node_color=node_colors.get(node, 'lightblue'),
                node_size=node_sizes.get(node, 1000),
                alpha=0.8
            )
        
        # 绘制边
        nx.draw_networkx_edges(
            G, pos,
            width=1.5,
            alpha=0.5,
            edge_color='gray',
            arrows=True,
            arrowstyle='-|>',
            arrowsize=15
        )
        
        # 绘制标签，确保使用支持中文的字体
        font_properties = {'family': 'SimHei'} if os.name == 'nt' else {'family': 'sans-serif'}
        
        nx.draw_networkx_labels(
            G, pos,
            font_size=12,
            font_weight='bold',
            font_family=font_properties['family']
        )
        
        plt.axis('off')  # 隐藏坐标轴
        
        # 添加图例
        legend_elements = [
            plt.Line2D([0], [0], marker='o', color='w', label='中心节点', 
                    markerfacecolor='red', markersize=15),
            plt.Line2D([0], [0], marker='o', color='w', label='主题节点', 
                    markerfacecolor=colors[0], markersize=15),
            plt.Line2D([0], [0], marker='o', color='w', label='文档节点', 
                    markerfacecolor='lightgray', markersize=15)
        ]
        plt.legend(handles=legend_elements, loc='lower right', fontsize=12)
        
        # 保存图形
        plt.tight_layout()
        plt.savefig(output_file, format='png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"思维导图已保存至 '{output_file}'")
        
        # 返回图形信息统计
        print(f"思维导图包含 {len(G.nodes())} 个节点和 {len(G.edges())} 条边")
        print(f"中心节点: 1")
        print(f"主题节点: {len(themes)}")
        print(f"文档节点: {len(self.documents)}")
# 爬虫程序，将网络上的pdf文件下载下来
def is_pdf_url(url):
    """
    检查URL是否指向PDF文件
    
    """
    # 检查URL路径是否以.pdf结尾
    if url.lower().endswith('.pdf'):
        return True
    
    # 尝试通过HEAD请求检查内容类型
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'
        }
        response = requests.head(url, headers=headers, timeout=10)
        content_type = response.headers.get('Content-Type', '').lower()
        return 'application/pdf' in content_type
    except Exception:
        return False

def download_pdf_directly(url, output_folder):
    """
    直接下载PDF文件
    
    """
    try:
        if not os.path.exists(output_folder):
            print(f"创建{output_folder}文件夹")
            os.makedirs(output_folder)
        parsed_url = urlparse(url)
        file_name = parsed_url.netloc.replace('.', '_')
        if parsed_url.path and parsed_url.path != '/':
            path_part = parsed_url.path.rstrip('/').replace('/', '_')
            if path_part.startswith('_'):
                path_part = path_part[1:]
            file_name += '_' + path_part
            
        output_path = os.path.join(output_folder, f"{file_name}.pdf")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'
        }
        response = requests.get(url, headers=headers, stream=True)
        
        if response.status_code == 200:
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            return True
        else:
            print(f"下载PDF时出错，HTTP状态码: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"下载PDF时发生错误: {e}")
        return False
def process_url_list(target_urls, output_folder="target_pdfs"):
    """
    处理URL列表，将每个URL转换为PDF
    
    """
    successful_downloads = []
    
    print(f"总共 {len(target_urls)} 个URL需要处理")
    
    for i, url in enumerate(target_urls):
        print(f"\n处理 ({i+1}/{len(target_urls)}): {url}")
        if not is_pdf_url(url):
            print(f"第{i+1}个url不是pdf文件")
            continue
        result = download_pdf_directly(url, output_folder)
        if result:
            successful_downloads.append(result)
    
    print(f"\n处理完成! 成功下载: {len(successful_downloads)}/{len(target_urls)}")
    return successful_downloads


def main():
    # 指定PDF文件夹路径（替换为实际路径）
    pdf_folder = "D:/北京大学/第四学期/导航项目"
    target_urls=["https://arxiv.org/pdf/2410.03761",
                 "https://arxiv.org/abs/2305.15186"
                 ]
    # 确保文件夹路径存在
    if not os.path.exists(pdf_folder):
        print(f"文件夹 '{pdf_folder}' 不存在，创建示例文件夹...")
        print("请修改代码中的pdf_folder变量为包含PDF文件的实际文件夹路径")
        return
    url_pdf_folder="D:\北京大学\第四学期\python程序设计\选做题\pdfs_url"
    process_url_list(target_urls,url_pdf_folder)
    # 创建PDF文档分析器
    analyzer = PDFDocumentAnalyzer(pdf_folder,url_pdf_folder)
    
    # 加载文档
    analyzer.load_documents(max_pages_per_doc=5)
    
    # 生成文档摘要
    analyzer.generate_document_summaries()
    
    # 比较文档
    comparison = analyzer.compare_documents()
    print("\n文档比较结果:")
    print(comparison)
    
    # 提取关键主题
    themes = analyzer.extract_key_themes()
    print("\n关键主题:")
    for theme in themes:
        print(f"- {theme}")
    
    # 进行特定问题的详细分析
    analysis = analyzer.get_detailed_analysis("这些文档中描述的方法有什么优缺点？")
    print("\n方法论分析:")
    print(analysis)
    
    # 生成思维导图
    analyzer.generate_mind_map(output_file="document_mind_map.png")
    
    # 生成完整报告
    analyzer.generate_report()

if __name__ == "__main__":
    main()
```

