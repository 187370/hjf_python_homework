# 第十次课后练习 之二（选做）

**负责助教：朱轩宇**

<span style="color:red; font-weight:bold;">请将作业文件命名为 第十次课后练习-选做题+姓名+学号.ipynb, 例如 第十次课后练习-选做题+张三+1000000000.ipynb</span>

# 社交媒体评论主题分析与可视化

## 引言
在信息化社会背景下，社交媒体平台成为了人们表达情感、分享观点的重要渠道。通过对社交媒体评论进行情感分析，可以深入了解用户的态度和情感趋势，为市场策略、用户体验优化等提供有力支持。本文将详细介绍如何使用Python进行社交媒体评论的情感分析，并结合数据可视化技术，将分析结果直观地呈现出来。

## 实验步骤
1. 数据清洗
   - 使用pandas读取数据文件，并进行数据清洗和预处理，包括去除重复值、正则清洗和分词。
2. 主要关注点分析
   - 计算词频并生成词云图，统计文本中词语的出现频率，并使用WordCloud库生成词云图展示结果。
3. 主题分析
   1. 进行一致性和困惑度计算，通过改变主题数量范围，计算不同主题数量下的一致性和困惑度，并绘制折线图展示结果。
   2. 使用TF-IDF模型提取文本的关键词，计算每个关键词在文本中的权重，并输出前30个关键词。
   3. 进行主题建模和关键词提取，使用LDA模型对分词结果进行主题建模，并提取每个主题的关键词。
   4. 对主题建模结果进行可视化，使用pyLDAvis库生成LDA主题模型的可视化结果，并保存为HTML文件。
   5. 聚类分析

实验提供了两份数据，一个比较大，一个比较小，可以先使用小数据跑通之后换更大的数据运行。

### 实验使用的关键库及版本
python 3.10
- jieba                     0.42.1
- numpy                     1.26.4
- pandas                    2.2.3
- pyLDAvis                  3.4.1
- scikit-learn              1.6.1
- seaborn                   0.13.2


## 1. 数据清洗

```python
import os
import pandas as pd
import re
import jieba
import jieba.posseg as psg
```

```python
# 使用pandas读取数据文件，并进行数据清洗和预处理，包括去除重复值、正则清洗和分词。
print("Loading data...")
data=pd.read_csv("./data/doc_ewujushi_short.csv")
stopword_list = []
with open("./stop_dic/stopwords.txt", "r", encoding="utf-8") as f:
    for line in f:
        stopword_list.append(line.strip())  # 去除换行符和空格
stopwords = set(stopword_list)

# 定义分词函数
def chinese_word_cut(mytext):
    ##############################
    # TODO 
    # 定义分词函数，实现去除非中文字符和停用词
    mytext =    # 去除非中文字符
    words =   # 使用jieba进行分词
    ##############################
    return " ".join([word for word, flag in words if word not in stopwords])  # 去除停用词

# 对数据进行处理
print("Processing data...")
##############################
# TODO 
# 对数据进行处理，包括去除重复值、缺失值以及重置索引
  # 去除重复值
  # 去除缺失值
  # 重置索引
##################################

print("Cleaning data...")
data["content_cutted"] = data.content.apply(chinese_word_cut)
```

## 2. 探索性数据分析(词频分析)

```python
word_count = {}
for index, row in data.iterrows():
    words = row["content_cutted"].split()
    for word in words:
        if word not in word_count:
            word_count[word] = 1
        else:
            word_count[word] += 1

# 排序
word_count_sorted = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
# 筛选高频词
high_frequency_num = 10  # 设定筛选的高频词数量
high_frequency_words = []
##############################
# TODO 
# 筛选高频词
high_frequency_words =
##############################
high_frequency_words
```

```python
# 可视化展示
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# 中文 字体设置
plt.rcParams['font.sans-serif']=['SimHei']
plt.rcParams['axes.unicode_minus']=False 
plt.rcParams.update({'font.size': 20}) # 设置字体大小

##############################
# TODO 
# 生成词频统计图，取前10个高频词
high_frequency_words_to_draw =  # 取前10个高频词
plt.figure(figsize=(10, 5))
plt.bar() # TODO
plt.xticks(rotation=45, ha="right")
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Word Frequency Statistics")
plt.tight_layout()
plt.show()
##############################
```

```python
##############################
# TODO 
# 生成词云图
wordcloud =
##############################
# 显示词云图
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```

## 3. LDA主题分析

```python
# LDA主题模型分析
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

##############################
# TODO 
# 使用CountVectorizer进行文本向量化
vectorizer = 
X = 

# 设定LDA模型参数，并进行拟合
n_components = 5  # 主题数量
lda = 
lda.fit(X)
##############################

# 输出主题词
def print_top_words(model, feature_names, n_top_words):
    tword = []
    for topic_idx, topic in enumerate(model.components_):
        print("Topic #%d:" % topic_idx)
        topic_w = " ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])
        tword.append(topic_w)
        print(topic_w)
    return tword

n_top_words = 25
tf_feature_names = vectorizer.get_feature_names_out()
topic_word = print_top_words(lda, tf_feature_names, n_top_words)
```

```python

import numpy as np

topics=lda.transform(X)
topic = []
##############################
# TODO 
# 得到每篇文章对应主题 
# 取主题概率最大的主题作为文章的主题

##############################
data['topic']=topic
data
```

```python
topics[0]#可以看出属于第几个话题的概率
```

## 可视化

```python
import pyLDAvis
import pyLDAvis.lda_model

pyLDAvis.enable_notebook()                                  #在notebook中展示   
pic = pyLDAvis.lda_model.prepare(lda, X, vectorizer)
pyLDAvis.save_html(pic, 'lda_pass'+str(n_components)+'.html')
```

## 困惑度分析

```python
import matplotlib.pyplot as plt
plexs = []
scores = []
n_max_topics = 10
#################################
# TODO
# 计算困惑度和对数似然函数
for i in range(1,n_max_topics):
    print('正在进行第',i,'轮计算')
    lda = 
    lda.fit(X)
    plexs.append(lda.perplexity(X))
    scores.append(lda.score(X))
#################################
```

```python
n_t=9                                      #区间最右侧的值。注意：不能大于n_max_topics
x=list(range(1,n_t))
plt.plot(x,plexs[1:n_t])
plt.xlabel("number of topics")
plt.ylabel("perplexity")
plt.show()
```

```python
n_t=9                                         #区间最右侧的值。注意：不能大于n_max_topics
x=list(range(1,n_t))
plt.plot(x,scores[1:n_t])
plt.xlabel("number of topics")
plt.ylabel("score")
plt.show()
```

## 4. 聚类分析

```python
# 聚类分析
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# 使用TF-IDF方法表示文本特征
vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words="english")
X = vectorizer.fit_transform(data["content_cutted"])

# 设定KMeans聚类参数
n_clusters = 3  # 聚类数量
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
kmeans.fit(X)

# 获取每个聚类的关键特征
def get_top_keywords(model, feature_names, n_top_words):
    
    top_keywords = []
    ##############################
    # TODO 
    # 实现函数，获取每个聚类的关键特征
    # 通过聚类中心的特征值来获取每个聚类的关键词
    
    ##############################
    return top_keywords

top_keywords = get_top_keywords(kmeans, vectorizer.get_feature_names_out(), 10)
for i, keywords in enumerate(top_keywords):
    print(f"Cluster {i}: {', '.join(keywords)}")

# 获取每个样本所属的聚类
data["cluster"] = kmeans.labels_
data["cluster"].value_counts()  # 查看每个聚类的样本数量

# 降维和可视化
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns

##############################
# TODO 
# 使用PCA降维到2维
pca = 
X_pca = pca.fit_transform(X.toarray())
##############################

# 创建DataFrame用于可视化
df = pd.DataFrame(X_pca, columns=["x", "y"])
df["cluster"] = data["cluster"]
df["topic"] = data["topic"]
# 可视化聚类结果
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x="x", y="y", hue="cluster", palette="Set1", alpha=0.7)
plt.title("KMeans Clustering Results")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend(title="Cluster")
plt.show()

```

### 请合理使用AI工具对上述媒体内容进行综述，给出你的步骤（调用方案）以及提示词和结果。

结果示例：

国际局势动荡下的全球经济与市场反应
——乌克兰危机升级，避险情绪推高黄金原油价格

近期，乌克兰局势持续紧张，俄罗斯与西方国家的对峙进一步升级，引发全球市场剧烈震荡。据央视新闻报道，乌克兰东部地区冲突加剧，美国总统拜登与北约盟国宣布对俄罗斯实施新一轮经济制裁，导致国际油价和黄金价格大幅上涨。

市场影响

能源市场：受紧张局势影响，布伦特原油价格突破每桶100美元，欧洲天然气价格飙升。分析人士指出，俄罗斯作为全球主要能源出口国，若冲突持续，可能进一步扰乱供应链。

避险资产：黄金价格创下近8个月新高，美元指数走强。金灿荣教授在接受采访时表示，市场避险情绪浓厚，投资者纷纷转向黄金等安全资产。

股市震荡：美股三大指数集体下跌，纳指跌幅超2%。A股市场同样受到波及，沪指失守3500点。

国际反应
俄罗斯总统普京在公开讲话中指责北约东扩威胁国家安全，而乌克兰政府则呼吁国际社会提供更多支持。微博上，#乌克兰局势#话题阅读量突破10亿，网友热议战争风险与全球政治格局变化。

社交媒体动态
与此同时，国内社交媒体呈现两极分化：

严肃讨论：部分用户转发央视新闻、专家解读，关注事件对经济的影响（如“黄金要不要现在买入？”）。

娱乐化消解：另一部分网友以“二哈表情包”“哈哈哈”调侃紧张氛围，甚至将普京称为“雄狮”，形成“悲允”式黑色幽默。

文化视角
在文学社区，有读者推荐三叔的《落魄》全文，称其“值得一看”，并关联当前局势：“历史总是惊人相似，如同苏联解体时的沙姆事件。”

编辑评述
本次乌克兰危机不仅是地缘政治冲突，更是一场全球经济的压力测试。从市场反应看，能源与金融市场的波动揭示了全球化时代危机的传导性。值得注意的是，社交媒体上的分裂态度——严肃关注与娱乐化解构并存，反映了公众对复杂议题的多元应对策略。

建议投资者：

短期关注避险资产（黄金、美元），但警惕高位回调风险；

长期需观察俄乌谈判进展及美联储政策变化。

社会观察：当“普京”与“二哈”同屏出现时，我们或许该思考：娱乐化表达是缓解焦虑的方式，还是对严肃议题的消解？

```python
# your solution


```

```python
# result

```

